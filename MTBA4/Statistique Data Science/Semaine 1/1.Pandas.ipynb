{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis with Pandas and Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Pandas` is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like .csv, .tsv, or .xlsx. Pandas makes it very convenient to load, process, and analyze such tabular data. Together with `Matplotlib` and `Seaborn`, `Pandas` provides a wide range of opportunities for visual analysis of tabular data.\n",
    "\n",
    "The main data structures in `Pandas` are implemented with **Series** and **DataFrame** classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure — a table — where each column contains data of the same type. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports to set up plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some nice visual settings\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# graphics in the \"retina\" format are more sharp and legible\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# setting the plot size and color scheme\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "plt.rcParams[\"image.cmap\"] = \"viridis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show the main methods in action by analyzing a [dataset on the churn rate](https://www.kaggle.com/datasets/spscientist/telecom-data) of telecom operator clients. A customer that is \"churning\" is a customer who terminates his/her contract with the company (typically because he/she is not satisfied with the service). The churn rate measures the operator's loss in customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data in the .csv format, we use the `read_csv()` method from Pandas. We then use the `head()` method to have a look at the first 5 lines of a new DataFrame (to make sure that everything is ok). (Note that you need to create a directory called \"data\" in your working directory (typically, it is the same directory where this notebook is located) and put telecom_churn.csv there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/telecom_churn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the data more closely. Using `shape`, we can see that the dataset contains 3333 rows and 20 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `columns`, we have access to column (feature) names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can use the `info()` method to output some general information about the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, for example, we can easily see that there are no missing values in the dataset (because each column contains 3333 values, the same number as the total number of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `describe()` shows some basic statistical characteristics for each numerical feature: number of non-missing values, mean, standard deviation, range, median, 0.25, and 0.75 quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical and boolean features, we can use `value_counts()`. Let's take a look at the distribution of *Churn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate ratios, we can pass `normalize=True` to `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Churn\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q1] What state is the most frequent US state in the dataset? How many different states are there in the dataset? Use `value_counts()` to answer these questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"State\"].value_counts() # -> WV    106 is the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and retrieving data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame can be indexed in a few different ways. To get a single column, we can use `DataFrame[\"Name\"]`. Let's use this to calculate the proportion of customers who have churned (i.e., `Churn == True`)? We can calculate it directly, without `value_counts()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Churn\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate of 14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q2] How much time, on average, do customers spend on the phone during daytime? Calculate mean and median values with the same type of command as above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Total day minutes\"].median()) # mean = 179.7 min, med =179.4"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient and widely used way of indexing in DataFrames is boolean indexing. The syntax is `df[predicate]`, where `predicate` is some logical expression for one or several columns checked for each element of these columns. The result of such indexing is the DataFrame consisting only of rows that satisfy the predicate condition. \n",
    "  \n",
    "Let's use this to calculate how much time, on average, the customers who have and have not churned spend on the phone during daytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Churn\"] == True][\"Total day minutes\"].mean(),\\\n",
    "df[df[\"Churn\"] == False][\"Total day minutes\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what is the maximum length of international calls among the customers that have not churned and do not have an international plan, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"Churn\"] == False) & (df[\"International plan\"] == \"No\")][\"Total intl minutes\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q3] What is the proportion of churned customers among those that have and do not have an international plan?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internationalPlan = df[(df[\"Churn\"] == True) & (df[\"International plan\"] == \"Yes\")].shape[0] / df[(df[\"International plan\"] == \"Yes\")].shape[0]\n",
    "noInternationalPlan = df[(df[\"Churn\"] == True) & (df[\"International plan\"] == \"No\")].shape[0] / df[(df[\"International plan\"] == \"No\")].shape[0]\n",
    "print(\"churned [%] with internationalPlan : \" + str(internationalPlan))\n",
    "print(\"churned [%] without internationalPlan : \" + str(noInternationalPlan))\n",
    "\n",
    "#churned proportion with internationalPlan : 0.4241486068111455\n",
    "# churned proportion without internationalPlan : 0.11495016611295682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q4] What is the proportion of churned customers among those that (1) have an international plan and (2) made at least 3 customer service calls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internationalPlanAndSupport = df[(df[\"Churn\"] == True) & (df[\"International plan\"] == \"Yes\") & (df[\"Customer service calls\"] >= 3)].shape[0] / df[(df[\"International plan\"] == \"Yes\")].shape[0]\n",
    "print(\"churned proportion with 3 support call and international plan : \" + str(internationalPlanAndSupport))\n",
    "#churned proportion with 3 support call and international plan : 0.10526315789473684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above can be done more effectively using grouping. In general, grouping data in Pandas works as follows:\n",
    "\n",
    "```python\n",
    "df.groupby(by=grouping_columns)[columns_to_show].function()\n",
    "```\n",
    "\n",
    "1. First, the `groupby()` method divides the `grouping_columns` by their values. The resulting DataFrame will be indexed by `grouping_columns`.\n",
    "2. Then, columns of interest are selected (`columns_to_show`). If `columns_to_show` is not specified, all non-groupby columns are included.\n",
    "3. Finally, one or several functions are applied to all the obtained groups (for each selected column).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where we group the data according to the values of *Churn* and display various statistics of two columns in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m columns_to_show \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal day minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal intl minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChurn\u001B[39m\u001B[38;5;124m\"\u001B[39m])[columns_to_show]\u001B[38;5;241m.\u001B[39mdescribe()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "columns_to_show = [\"Total day minutes\", \"Total intl minutes\"]\n",
    "\n",
    "df.groupby([\"Churn\"])[columns_to_show].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_show = [\"Total day minutes\", \"Total intl minutes\"]\n",
    "\n",
    "df.groupby([\"Churn\"])[columns_to_show].agg([np.mean, np.std, np.min, np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q5] Display similarly the same statistics for *Total eve minutes* and *Total night minutes*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codecolumns_to_show = [\"Total eve minutes\", \"Total night minutes\"]\n",
    "print(df.groupby([\"Churn\"])[columns_to_show].describe())\n",
    "print(df.groupby([\"Churn\"])[columns_to_show].agg([np.mean, np.std, np.min, np.max])) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to see how the observations in our dataset are distributed in the context of two variables — *Churn* and *International plan*. To do so, we can build a contingency table using `crosstab()`.\n",
    "In statistics, a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays how often a pair of variables take a specific value in a dataset. It provides a basic picture of the interrelation between two variables and can help find interactions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"Churn\"], df[\"International plan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q6] Build a \"normalized\" contingency table for *Churn* and *Voice mail plan* with ratios instead of counts. To do this, pass `normalize=True` to `crosstab()`. Read more about other possible ways to normalize crosstabs at [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df[\"Churn\"], df[\"Voice mail plan\"], normalize=True))\n",
    "\n",
    "\n",
    "#Voice mail plan    No   Yes\n",
    "#Churn\n",
    "#False            0.60  0.25\n",
    "#True             0.12  0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q7] Build a contingency table for *Churn* and *Customer service calls*. Add row and column subtotals to this table. To find out how to do this, use the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df[\"Churn\"], df[\"Customer service calls\"], normalize=True,margins=True))\n",
    "\n",
    "# Customer service calls     0     1     2     3     4         5         6         7         8         9   All\n",
    "#Churn\n",
    "#False                   0.18  0.32  0.20  0.12  0.03  7.80e-03  2.40e-03  1.20e-03  3.00e-04  0.00e+00  0.86\n",
    "#True                    0.03  0.04  0.03  0.01  0.02  1.20e-02  4.20e-03  1.50e-03  3.00e-04  6.00e-04  0.14\n",
    "#All                     0.21  0.35  0.23  0.13  0.05  1.98e-02  6.60e-03  2.70e-03  6.00e-04  6.00e-04  1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting observation:** We can see that most of the customers are loyal and do not use additional services (International Plan/Voice mail). It can also be seen that the churn rate increases sharply starting from 4 customer service calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual analysis (and first attempt at predicting telecom churn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss a number of possible ways to visualize the data and the results we obtained above. For these purposes, we will use `Seaborn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how *Сhurn* is related to *International plan* we can use `countplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"International plan\", hue=\"Churn\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clearly see that, with an international plan, the churn rate is much higher. Perhaps large and poorly controlled expenses with international calls are very conflict-prone and lead to dissatisfaction among customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q8] Make a similar plot for *Customer service calls*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Customer service calls\", hue=\"Churn\", data=df).get_figure().savefig(\"out.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, the churn rate increases sharply starting from 4 customer service calls. It is much easier to notice it on the chart! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a new binary feature to our DataFrame: `Many customer service calls = Customer service calls > 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Many customer service calls\"] = df[\"Customer service calls\"] > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q9] Using the crosstab and countplot, show how *Many_service_calls* relates to Churn.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Many customer service calls\"] = df[\"Customer service calls\"] > 3\n",
    "print(pd.crosstab(df[\"Churn\"], df[\"Many customer service calls\"], margins=True))\n",
    "sns.countplot(x=\"Many customer service calls\", hue=\"Churn\", data=df).get_figure().savefig(\"out2.png\")\n",
    "\n",
    "#Many customer service calls  False  True   All\n",
    "#Churn\n",
    "#False                         2721   129  2850\n",
    "#True                           345   138   483\n",
    "#All                           3066   267  3333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make another contingency table that relates *Churn* with both *International plan* and *Many_service_calls*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab((df[\"Many customer service calls\"]) & (df[\"International plan\"]=='Yes'), df[\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On the prediction of churned customers:** We can see from above that by predicting that a customer will terminate his/her contract with the company (i.e., *Churn*=1) in the case when (1) *Customer service calls* is greater than 3 and (2) the *International Plan* is True (and predicting *Churn*=0 otherwise), we might rarely be wrong. Namely, for the customers in our dataset, we will make a mistake only $464 + 9 = 473$ times. Speaking in terms of Machine Learning lingo, we might expect an accuracy of $1-473/3333=0.858$ (accuracy score is an evaluation metric that measures the number of correct predictions made by a model in relation to the total number of predictions made; it is calculated by dividing the number of correct predictions by the total number of predictions). This number, $85.8\\%$, that we got through this simple reasoning can serve as a good starting point (baseline) for further models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other plots that might be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we show other possible graphs that can be plotted with `Seaborn` and `Pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with histograms. A histogram is a classic visualization tool that represents the distribution of one or more variables by counting the number of observations that fall within discrete bins. It is often used to illustrate the major features of the distribution of the data in a convenient form. The shape of a histogram provides clues about the underlying distribution type. To build a histogram for one or several features, we can use the method `hist()` directly from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_show = [\"Total day minutes\", \"Total intl calls\"]\n",
    "df[columns_to_show].hist(figsize=(9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q10] Use the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) to find how to change the number of bins in the histograms from above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_show = [\"Total day minutes\", \"Total intl calls\"]\n",
    "(df[columns_to_show].hist(figsize=(9,3), bins=10))[0][0].get_figure().savefig('out30.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build histograms, we can also use `histplot()` from Seaborn. This function can normalize the statistic computed within each bin to estimate frequency, density, or probability mass (we specify it in `stat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(9,3))\n",
    "\n",
    "sns.histplot(data=df[\"Total day calls\"], ax=axes[0], bins=20, stat=\"density\");\n",
    "sns.histplot(data=df[\"Total intl calls\"], ax=axes[1], bins=10, stat=\"density\");\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also draw box plots for various features using Seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(1,2, figsize=(9,3))\n",
    "\n",
    "sns.boxplot(data=df[\"Total day calls\"], ax=axes[0]);\n",
    "sns.boxplot(data=df[\"Total intl calls\"], ax=axes[1]);\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the relationship between two features, we can use the scatter plot. The scatter plot graphs pairs of numerical data, with one variable on each axis, to look for a relationship between them. If the variables are correlated (we will discuss the correlation later in the course), the points will fall along a line or curve. The better the correlation, the tighter the points will hug the line. \n",
    "\n",
    "In Seaborn, `scatterplot()` can be used to make a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"Total day minutes\", y=\"Total night minutes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the points of a scatter plot can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can use the parameter `hue` to indicate our categorical feature of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x=\"Total day minutes\", y=\"Total night minutes\", hue=\"Churn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q11] We can see that the size of markers in the scatter plots is quite large and this complicates their interpretation. Use the [Seaborn documentation](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) or Google to find out how to change the marker size in the scatterplots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x=\"Total day minutes\", y=\"Total night minutes\", hue=\"Churn\", size=1).get_figure().savefig(\"out41.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the correlation between several features in a DataFrame, we can use the correlation matrix. Loosely speaking, this matrix shows the correlation values, which measure the degree of linear relationship between each pair of features (but we will discuss it in more detail later in the course). The correlation matrix is used to summarize data, as input into a more advanced analysis, and as a diagnostic for advanced analyses.\n",
    "\n",
    "Below, we first drop some of the features (calls and minutes duplicate charges), then we use the method `corr()` from Pandas that calculates the correlation between each pair of features (at this step, non-numerical features are excluded automatically), and finally, we pass the resulting correlation matrix to `heatmap()` from Seaborn, which renders a color-coded matrix for the provided values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = set(df.columns)\n",
    "features_to_exclude = set([\"Total day calls\",\n",
    "                        \"Total day minutes\",\n",
    "                        \"Total eve calls\",\n",
    "                        \"Total eve minutes\",\n",
    "                        \"Total night calls\",\n",
    "                        \"Total night minutes\",\n",
    "                        \"Total intl calls\",\n",
    "                        \"Total intl minutes\"])\n",
    "\n",
    "features_to_show = list(all_features-features_to_exclude)\n",
    "corr_matrix = df[features_to_show].corr()\n",
    "sns.heatmap(corr_matrix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this chart that *Churn* correlates the most with *Total day charge* and *Customer service calls* (and our *Many customer service calls*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
